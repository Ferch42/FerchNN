{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import FerchNN\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"MountainCarContinuous-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_nn = FerchNN.NN_Network(input_size = env.observation_space.shape[0], lr =0.000001, output_size = env.action_space.shape[0] ,  activation = \"linear\")\n",
    "std_nn = FerchNN.NN_Network(input_size = env.observation_space.shape[0], lr =0.000001, output_size = env.action_space.shape[0] , activation = \"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timee = 0\n",
    "def std_transformation(std, c = 500):\n",
    "    global timee\n",
    "    return np.power(std,2)+4*np.power(np.e, -(timee/c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    \n",
    "    action = np.zeros(env.action_space.shape[0])\n",
    "    mean = mean_nn.predict(state)\n",
    "    std = std_nn.predict(state)\n",
    "    #std_aux= np.log(1+np.power(np.e, std))+0.00001\n",
    "    std_aux = std_transformation(std)\n",
    "    #std_aux = np.zeros(env.action_space.shape)\n",
    "    \n",
    "    for i in range(env.action_space.shape[0]):\n",
    "        \n",
    "        action[i] = np.random.normal(mean[i], std_aux[i])\n",
    "        \n",
    "        if action[i]>= env.action_space.high[i]:\n",
    "            action[i] = env.action_space.high[i]\n",
    "        \n",
    "        if action[i]<= env.action_space.low[i]:\n",
    "            action[i] = env.action_space.low[i]\n",
    "            \n",
    "        \n",
    "    \n",
    "    #std = std_aux\n",
    "    #print(mean, std_aux)\n",
    "    return action, mean, std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(action, reward, mean, std):\n",
    "    \n",
    "    #std_aux= np.log(1+np.power(np.e, std))+0.00001\n",
    "    std_aux = std_transformation(std)\n",
    "    mean_gradient = -reward*((action - mean)/np.power(std_aux,2))\n",
    "    #mean_gradient = (mean_gradient)/np.linalg.norm(mean_gradient)\n",
    "     \n",
    "    #std_gradient = -reward*((np.power((action-mean), 2)-np.power(std_aux,2))/np.power(std_aux,3))*( 1/ (1+np.power(np.e,-std)))\n",
    "    std_gradient = -reward*((np.power((action-mean), 2)-np.power(std_aux,2))/np.power(std_aux,3))*2*std\n",
    "    #std_gradient = (std_gradient)/ np.linalg.norm(std_gradient)\n",
    "    \n",
    "    \n",
    "    return mean_gradient, std_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cumulative_rewards(rewards, gamma = 0.99):\n",
    "    \n",
    "    r_aux = np.zeros(len(rewards))\n",
    "    \n",
    "    for i in range(len(rewards)):\n",
    "        discounted_rewards = 0\n",
    "        \n",
    "        for j in range(i, len(rewards)): \n",
    "            discounted_rewards = discounted_rewards + np.power(gamma, j-i)*rewards[j]\n",
    "        \n",
    "        r_aux [i] = discounted_rewards\n",
    "    \n",
    "    return r_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reinforce(actions, means, stds, rewards):\n",
    "    \n",
    "    r = compute_cumulative_rewards(rewards)\n",
    "    \n",
    "    assert len(actions)== len(means)\n",
    "    assert len(means) == len(stds)\n",
    "    assert len(stds) == len(rewards)\n",
    "    \n",
    "    for i in range(len(actions)):\n",
    "        mean_gradient, std_gradient  = compute_gradient(actions[i], r[i], means[i], stds[i])\n",
    "        mean_nn.fit(None,None, dloss = mean_gradient)\n",
    "        std_nn.fit(None, None, dloss = std_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Fernando\\Documents\\TCC\\continuous_action_space\\FerchNN.py:210: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  if dloss != None:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "0  Reward  -99.8057024654\n",
      "0  Mean  29.8496860894\n",
      "0  Std  881.229867205\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "1  Reward  -99.9\n",
      "1  Mean  29.853373261\n",
      "1  Std  881.459293862\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "2  Reward  -99.8332720351\n",
      "2  Mean  29.8533742482\n",
      "2  Std  881.48714898\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "3  Reward  -99.8909618718\n",
      "3  Mean  29.853374593\n",
      "3  Std  881.514960445\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "4  Reward  -99.8091358342\n",
      "4  Mean  29.8533749388\n",
      "4  Std  881.542809671\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "5  Reward  -99.8987303559\n",
      "5  Mean  29.8533752838\n",
      "5  Std  881.570643035\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "6  Reward  -99.8505672393\n",
      "6  Mean  29.8533756297\n",
      "6  Std  881.598527698\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "7  Reward  -99.8201313454\n",
      "7  Mean  29.8533759753\n",
      "7  Std  881.626409346\n",
      "-------------------------------\n",
      "Episode finished after 887 timesteps\n",
      "8  Reward  11.4\n",
      "8  Mean  29.8533763201\n",
      "8  Std  881.654295186\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "9  Reward  -86.8849293828\n",
      "9  Mean  -0.146570538955\n",
      "9  Std  4.06836463565\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "10  Reward  -88.2177943888\n",
      "10  Mean  -0.146605337063\n",
      "10  Std  4.06153512168\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "11  Reward  -85.8331552654\n",
      "11  Mean  -0.146663523611\n",
      "11  Std  4.05474038599\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "12  Reward  -86.1637797197\n",
      "12  Mean  -0.14670644988\n",
      "12  Std  4.04794430891\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "13  Reward  -85.1296265276\n",
      "13  Mean  -0.146753443735\n",
      "13  Std  4.04118002125\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "14  Reward  -86.7858236875\n",
      "14  Mean  -0.146784455133\n",
      "14  Std  4.03443047387\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "15  Reward  -87.0045861762\n",
      "15  Mean  -0.146840034107\n",
      "15  Std  4.02772149446\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "16  Reward  -86.0540097001\n",
      "16  Mean  -0.146879374971\n",
      "16  Std  4.02103800402\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "17  Reward  -86.0441163758\n",
      "17  Mean  -0.146938583233\n",
      "17  Std  4.01436839493\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "18  Reward  -86.2457799364\n",
      "18  Mean  -0.147013287692\n",
      "18  Std  4.00772274113\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "19  Reward  -88.3673654093\n",
      "19  Mean  -0.147050903652\n",
      "19  Std  4.0011117624\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "20  Reward  -85.1527854569\n",
      "20  Mean  -0.147078979274\n",
      "20  Std  3.99455155039\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "21  Reward  -86.8324231884\n",
      "21  Mean  -0.147132561582\n",
      "21  Std  3.98797584492\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "22  Reward  -86.2801114071\n",
      "22  Mean  -0.147203833657\n",
      "22  Std  3.98144235264\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "23  Reward  -86.8785473102\n",
      "23  Mean  -0.147286505946\n",
      "23  Std  3.97492799156\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "24  Reward  -85.3156035707\n",
      "24  Mean  -0.147357284825\n",
      "24  Std  3.96845123433\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "25  Reward  -85.8556333589\n",
      "25  Mean  -0.147389614575\n",
      "25  Std  3.9619792335\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "26  Reward  -87.0190864474\n",
      "26  Mean  -0.147448905004\n",
      "26  Std  3.955538503\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "27  Reward  -87.3674126853\n",
      "27  Mean  -0.147528319844\n",
      "27  Std  3.94913593698\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "28  Reward  -85.8977330365\n",
      "28  Mean  -0.147571632038\n",
      "28  Std  3.94276509604\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "29  Reward  -83.2051845911\n",
      "29  Mean  -0.147612830039\n",
      "29  Std  3.93639930621\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "30  Reward  -85.9039584255\n",
      "30  Mean  -0.147694506726\n",
      "30  Std  3.93002539108\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "31  Reward  -84.7688265751\n",
      "31  Mean  -0.147738187254\n",
      "31  Std  3.92370931719\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "32  Reward  -87.7024690249\n",
      "32  Mean  -0.147814264831\n",
      "32  Std  3.91740319414\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "33  Reward  -87.234201352\n",
      "33  Mean  -0.147894325918\n",
      "33  Std  3.91116333633\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "34  Reward  -86.3491770668\n",
      "34  Mean  -0.14797765903\n",
      "34  Std  3.90494829559\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "35  Reward  -87.3876255506\n",
      "35  Mean  -0.148021792858\n",
      "35  Std  3.89874093176\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "36  Reward  -85.9302783108\n",
      "36  Mean  -0.148083738544\n",
      "36  Std  3.89257757988\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "37  Reward  -87.2811157458\n",
      "37  Mean  -0.148129115921\n",
      "37  Std  3.88642388409\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "38  Reward  -86.7579973712\n",
      "38  Mean  -0.148188109993\n",
      "38  Std  3.88031276982\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "39  Reward  -86.2607372158\n",
      "39  Mean  -0.1482798769\n",
      "39  Std  3.87422072408\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "40  Reward  -86.5603263523\n",
      "40  Mean  -0.148339027217\n",
      "40  Std  3.86814841569\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "41  Reward  -87.1061388573\n",
      "41  Mean  -0.14839975287\n",
      "41  Std  3.86211520052\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "42  Reward  -85.3233726744\n",
      "42  Mean  -0.14846037641\n",
      "42  Std  3.85611537732\n",
      "-------------------------------\n",
      "Episode finished after 875 timesteps\n",
      "43  Reward  24.5185591731\n",
      "43  Mean  -0.148521193865\n",
      "43  Std  3.85011645945\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "44  Reward  -85.8760196856\n",
      "44  Mean  -0.148371387882\n",
      "44  Std  3.8422157862\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "45  Reward  -87.3204901715\n",
      "45  Mean  -0.14840765311\n",
      "45  Std  3.83625069798\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "46  Reward  -85.6871336372\n",
      "46  Mean  -0.148454201209\n",
      "46  Std  3.83034290615\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "47  Reward  -83.9201124699\n",
      "47  Mean  -0.148508512593\n",
      "47  Std  3.824435003\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "48  Reward  -85.5585855585\n",
      "48  Mean  -0.148539625826\n",
      "48  Std  3.8185321261\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "49  Reward  -86.5198290317\n",
      "49  Mean  -0.148606915848\n",
      "49  Std  3.81267461278\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "50  Reward  -85.9863262787\n",
      "50  Mean  -0.148658479908\n",
      "50  Std  3.80686695644\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "51  Reward  -85.8158518812\n",
      "51  Mean  -0.148731456049\n",
      "51  Std  3.8010731029\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "52  Reward  -87.3219832848\n",
      "52  Mean  -0.14879609759\n",
      "52  Std  3.79530100509\n",
      "-------------------------------\n",
      "Episode finished after 494 timesteps\n",
      "53  Reward  57.3101172681\n",
      "53  Mean  -0.148888250107\n",
      "53  Std  3.78958229597\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "54  Reward  -86.2069579233\n",
      "54  Mean  -0.148773953332\n",
      "54  Std  3.78116135886\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "55  Reward  -85.4755664979\n",
      "55  Mean  -0.148796834283\n",
      "55  Std  3.77546272825\n",
      "-------------------------------\n",
      "Episode finished after 999 timesteps\n",
      "56  Reward  13.487158474\n",
      "56  Mean  -0.148894138157\n",
      "56  Std  3.76978068675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "57  Reward  -85.6959544982\n",
      "57  Mean  -0.14881573261\n",
      "57  Std  3.76221164189\n",
      "-------------------------------\n",
      "Episode finished after 907 timesteps\n",
      "58  Reward  24.5756744298\n",
      "58  Mean  -0.148872034125\n",
      "58  Std  3.75657662175\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "59  Reward  -85.5761302081\n",
      "59  Mean  -0.148821919046\n",
      "59  Std  3.74881996047\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "60  Reward  -83.6598708673\n",
      "60  Mean  -0.148878598505\n",
      "60  Std  3.74321596429\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "61  Reward  -85.5491717614\n",
      "61  Mean  -0.148936307061\n",
      "61  Std  3.73761797311\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "62  Reward  -84.465518305\n",
      "62  Mean  -0.149019506912\n",
      "62  Std  3.73206413119\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "63  Reward  -85.4151216016\n",
      "63  Mean  -0.149108173268\n",
      "63  Std  3.72652584439\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "64  Reward  -85.6291555754\n",
      "64  Mean  -0.149160171959\n",
      "64  Std  3.72103049067\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "65  Reward  -84.2293795575\n",
      "65  Mean  -0.149226105507\n",
      "65  Std  3.71556600561\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "66  Reward  -84.6241148991\n",
      "66  Mean  -0.149297143536\n",
      "66  Std  3.71010521253\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "67  Reward  -85.2145456964\n",
      "67  Mean  -0.14937240925\n",
      "67  Std  3.70467681434\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "68  Reward  -87.3546989004\n",
      "68  Mean  -0.149433088118\n",
      "68  Std  3.69928683033\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "69  Reward  -85.1192433096\n",
      "69  Mean  -0.149458085831\n",
      "69  Std  3.69396688043\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "70  Reward  -86.0241502067\n",
      "70  Mean  -0.149543891357\n",
      "70  Std  3.68862853605\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "71  Reward  -83.9495056805\n",
      "71  Mean  -0.149624434791\n",
      "71  Std  3.68334249045\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "72  Reward  -84.9819581846\n",
      "72  Mean  -0.149672213811\n",
      "72  Std  3.67804494761\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "73  Reward  -83.8789779448\n",
      "73  Mean  -0.149748743139\n",
      "73  Std  3.67279262246\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "74  Reward  -86.4868077867\n",
      "74  Mean  -0.14981169252\n",
      "74  Std  3.66755244547\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "75  Reward  -86.3672490019\n",
      "75  Mean  -0.149879094882\n",
      "75  Std  3.66239631855\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "76  Reward  -83.6967629555\n",
      "76  Mean  -0.14998237272\n",
      "76  Std  3.6572685086\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "77  Reward  -85.7313553572\n",
      "77  Mean  -0.150069923907\n",
      "77  Std  3.6521095748\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "78  Reward  -85.9488176642\n",
      "78  Mean  -0.150137889441\n",
      "78  Std  3.64702517674\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "79  Reward  -84.8996790356\n",
      "79  Mean  -0.15018504708\n",
      "79  Std  3.64197480897\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "80  Reward  -86.5726002247\n",
      "80  Mean  -0.150247362581\n",
      "80  Std  3.6369333696\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "81  Reward  -85.3461338776\n",
      "81  Mean  -0.15035652594\n",
      "81  Std  3.63195125322\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "82  Reward  -86.3554857484\n",
      "82  Mean  -0.150377761227\n",
      "82  Std  3.62698479332\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "83  Reward  -86.4134519688\n",
      "83  Mean  -0.150446606816\n",
      "83  Std  3.62205694818\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "84  Reward  -84.0983740327\n",
      "84  Mean  -0.150499498324\n",
      "84  Std  3.61716180809\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "85  Reward  -85.4570544491\n",
      "85  Mean  -0.150551606517\n",
      "85  Std  3.61226359303\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "86  Reward  -86.9136516186\n",
      "86  Mean  -0.150623530466\n",
      "86  Std  3.60742460057\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "87  Reward  -84.6726751708\n",
      "87  Mean  -0.150709587682\n",
      "87  Std  3.60263501336\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "88  Reward  -85.7049933133\n",
      "88  Mean  -0.150795145163\n",
      "88  Std  3.5978332457\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "89  Reward  -85.5965921368\n",
      "89  Mean  -0.150865656976\n",
      "89  Std  3.59308723795\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "90  Reward  -85.6698047627\n",
      "90  Mean  -0.15095389057\n",
      "90  Std  3.58836111778\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "91  Reward  -84.4025438374\n",
      "91  Mean  -0.151010366581\n",
      "91  Std  3.58367380398\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "92  Reward  -85.8803338266\n",
      "92  Mean  -0.151084623809\n",
      "92  Std  3.57898987104\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "93  Reward  -84.8964383198\n",
      "93  Mean  -0.151156012841\n",
      "93  Std  3.57437176933\n",
      "-------------------------------\n",
      "Episode finished after 943 timesteps\n",
      "94  Reward  20.1038918913\n",
      "94  Mean  -0.151218082344\n",
      "94  Std  3.56975451474\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "95  Reward  -85.54928408\n",
      "95  Mean  -0.151131874106\n",
      "95  Std  3.56238035489\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "96  Reward  -85.1229395158\n",
      "96  Mean  -0.151188424092\n",
      "96  Std  3.55783444346\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "97  Reward  -85.2213069236\n",
      "97  Mean  -0.151260842177\n",
      "97  Std  3.55330865855\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "98  Reward  -85.7345920267\n",
      "98  Mean  -0.151309548033\n",
      "98  Std  3.54880781073\n",
      "-------------------------------\n",
      "Episode finished after 627 timesteps\n",
      "99  Reward  46.7987086216\n",
      "99  Mean  -0.151374793194\n",
      "99  Std  3.544358499\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "100  Reward  -85.0979718035\n",
      "100  Mean  -0.151307982762\n",
      "100  Std  3.53628020603\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "101  Reward  -84.8340488135\n",
      "101  Mean  -0.151386376369\n",
      "101  Std  3.53185135624\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "102  Reward  -84.5036734229\n",
      "102  Mean  -0.151456476973\n",
      "102  Std  3.52744574393\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "103  Reward  -85.0209169454\n",
      "103  Mean  -0.151519665669\n",
      "103  Std  3.52305735292\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "104  Reward  -84.140022366\n",
      "104  Mean  -0.151621547901\n",
      "104  Std  3.51872786324\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "105  Reward  -84.3197541175\n",
      "105  Mean  -0.151691709304\n",
      "105  Std  3.51439594678\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "106  Reward  -84.952621814\n",
      "106  Mean  -0.151798229926\n",
      "106  Std  3.51011639742\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "107  Reward  -83.8047623043\n",
      "107  Mean  -0.151886105492\n",
      "107  Std  3.50587925836\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "108  Reward  -85.8756525126\n",
      "108  Mean  -0.151985782379\n",
      "108  Std  3.50165062113\n",
      "-------------------------------\n",
      "Episode finished after 736 timesteps\n",
      "109  Reward  37.600561608\n",
      "109  Mean  -0.152030245188\n",
      "109  Std  3.49751001525\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "110  Reward  -85.5924861446\n",
      "110  Mean  -0.151985971017\n",
      "110  Std  3.48970742948\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "111  Reward  -83.6258403056\n",
      "111  Mean  -0.152085084013\n",
      "111  Std  3.48560218495\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "112  Reward  -84.2063084447\n",
      "112  Mean  -0.152160042609\n",
      "112  Std  3.48147552235\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "113  Reward  -83.3259258065\n",
      "113  Mean  -0.152228461841\n",
      "113  Std  3.47740278799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "114  Reward  -83.8215963865\n",
      "114  Mean  -0.152276381191\n",
      "114  Std  3.47332153775\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "115  Reward  -83.4034475056\n",
      "115  Mean  -0.152358716914\n",
      "115  Std  3.46929231987\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "116  Reward  -86.3145998175\n",
      "116  Mean  -0.152408916909\n",
      "116  Std  3.46530640216\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "117  Reward  -85.4211029737\n",
      "117  Mean  -0.152450535627\n",
      "117  Std  3.46142297301\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "118  Reward  -85.7594749543\n",
      "118  Mean  -0.152514545203\n",
      "118  Std  3.45754082939\n",
      "-------------------------------\n",
      "Episode finished after 821 timesteps\n",
      "119  Reward  30.5862153097\n",
      "119  Mean  -0.152618565675\n",
      "119  Std  3.45370708632\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "120  Reward  -84.9649854732\n",
      "120  Mean  -0.15254486483\n",
      "120  Std  3.44615463632\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "121  Reward  -85.1130559203\n",
      "121  Mean  -0.152624302856\n",
      "121  Std  3.44233963924\n",
      "-------------------------------\n",
      "Episode finished after 641 timesteps\n",
      "122  Reward  42.8519058367\n",
      "122  Mean  -0.152674796368\n",
      "122  Std  3.43857059749\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "123  Reward  -83.1463439208\n",
      "123  Mean  -0.152474280035\n",
      "123  Std  3.43064847696\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "124  Reward  -83.219201422\n",
      "124  Mean  -0.152520827878\n",
      "124  Std  3.42685614569\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "125  Reward  -83.7770790945\n",
      "125  Mean  -0.152604144529\n",
      "125  Std  3.42310778929\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "126  Reward  -83.5981262314\n",
      "126  Mean  -0.152654483997\n",
      "126  Std  3.41940657949\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "127  Reward  -84.7716093096\n",
      "127  Mean  -0.152734742927\n",
      "127  Std  3.41572956538\n",
      "-------------------------------\n",
      "Episode finished after 898 timesteps\n",
      "128  Reward  23.61861302\n",
      "128  Mean  -0.152838329333\n",
      "128  Std  3.41211712316\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "129  Reward  -85.8360634345\n",
      "129  Mean  -0.152735556458\n",
      "129  Std  3.40483704632\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "130  Reward  -84.4036826723\n",
      "130  Mean  -0.152780541917\n",
      "130  Std  3.4013125557\n",
      "-------------------------------\n",
      "Episode finished after 583 timesteps\n",
      "131  Reward  50.7287846147\n",
      "131  Mean  -0.152873528545\n",
      "131  Std  3.39778213847\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "132  Reward  -83.6728311409\n",
      "132  Mean  -0.152710215973\n",
      "132  Std  3.38960128942\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "133  Reward  -82.989787418\n",
      "133  Mean  -0.152789425629\n",
      "133  Std  3.38607466113\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "134  Reward  -86.6648982332\n",
      "134  Mean  -0.152875150778\n",
      "134  Std  3.3825707695\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "135  Reward  -85.0388836318\n",
      "135  Mean  -0.152929773188\n",
      "135  Std  3.37921954722\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "136  Reward  -85.1088177509\n",
      "136  Mean  -0.153014404425\n",
      "136  Std  3.37584258661\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "137  Reward  -84.230820367\n",
      "137  Mean  -0.153106690042\n",
      "137  Std  3.37251543975\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "138  Reward  -85.2597401872\n",
      "138  Mean  -0.15316504737\n",
      "138  Std  3.36921212141\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "139  Reward  -84.0685767481\n",
      "139  Mean  -0.153209973148\n",
      "139  Std  3.36597672135\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "140  Reward  -83.2925492364\n",
      "140  Mean  -0.153324332981\n",
      "140  Std  3.36273760975\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "141  Reward  -85.9963636982\n",
      "141  Mean  -0.153406519059\n",
      "141  Std  3.35952708976\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "142  Reward  -86.3383550727\n",
      "142  Mean  -0.153475227857\n",
      "142  Std  3.35641998766\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "143  Reward  -82.127827915\n",
      "143  Mean  -0.153553891623\n",
      "143  Std  3.35337511838\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "144  Reward  -85.5934233242\n",
      "144  Mean  -0.153654039379\n",
      "144  Std  3.35021199777\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "145  Reward  -84.6408016962\n",
      "145  Mean  -0.153749603746\n",
      "145  Std  3.34720326903\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "146  Reward  -85.1386959527\n",
      "146  Mean  -0.153818149716\n",
      "146  Std  3.34421959061\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "147  Reward  -84.0734186321\n",
      "147  Mean  -0.153907738963\n",
      "147  Std  3.34128211966\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "148  Reward  -83.514708556\n",
      "148  Mean  -0.15398393071\n",
      "148  Std  3.33836363453\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "149  Reward  -85.3477959937\n",
      "149  Mean  -0.154056957829\n",
      "149  Std  3.33545710504\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "150  Reward  -84.1771490728\n",
      "150  Mean  -0.154125459002\n",
      "150  Std  3.33265475444\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "151  Reward  -85.438879226\n",
      "151  Mean  -0.154230051443\n",
      "151  Std  3.32984607039\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "152  Reward  -83.3750625642\n",
      "152  Mean  -0.154319682045\n",
      "152  Std  3.32712474274\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "153  Reward  -82.8022683377\n",
      "153  Mean  -0.154389094229\n",
      "153  Std  3.32437588531\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "154  Reward  -83.5479519429\n",
      "154  Mean  -0.154455738595\n",
      "154  Std  3.32163455147\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "155  Reward  -82.6986853046\n",
      "155  Mean  -0.154526436795\n",
      "155  Std  3.31896293042\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "156  Reward  -82.5032742234\n",
      "156  Mean  -0.154596616437\n",
      "156  Std  3.31629183934\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "157  Reward  -82.7770606101\n",
      "157  Mean  -0.154660347964\n",
      "157  Std  3.31366425768\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "158  Reward  -86.4500773379\n",
      "158  Mean  -0.154731019405\n",
      "158  Std  3.31108696929\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "159  Reward  -83.810703827\n",
      "159  Mean  -0.154839527167\n",
      "159  Std  3.30868172036\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "160  Reward  -84.3670767692\n",
      "160  Mean  -0.154930733284\n",
      "160  Std  3.30622593974\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "161  Reward  -84.6597777352\n",
      "161  Mean  -0.155015558475\n",
      "161  Std  3.30382254707\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "162  Reward  -84.9551721039\n",
      "162  Mean  -0.155087034954\n",
      "162  Std  3.30147423812\n",
      "-------------------------------\n",
      "Episode finished after 923 timesteps\n",
      "163  Reward  24.0416661256\n",
      "163  Mean  -0.155214459942\n",
      "163  Std  3.29918695405\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "164  Reward  -82.9935959337\n",
      "164  Mean  -0.155175982272\n",
      "164  Std  3.29194649044\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "165  Reward  -83.8663180361\n",
      "165  Mean  -0.15523646477\n",
      "165  Std  3.289641728\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "166  Reward  -82.9453012649\n",
      "166  Mean  -0.155315574068\n",
      "166  Std  3.28741738188\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "167  Reward  -83.4801062653\n",
      "167  Mean  -0.155408329083\n",
      "167  Std  3.28518474039\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "168  Reward  -83.4003478625\n",
      "168  Mean  -0.15544110996\n",
      "168  Std  3.28301730058\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "169  Reward  -84.6990137256\n",
      "169  Mean  -0.155561887491\n",
      "169  Std  3.28088191928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "170  Reward  -83.9584318865\n",
      "170  Mean  -0.155655599717\n",
      "170  Std  3.2788744801\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "171  Reward  -83.8474080884\n",
      "171  Mean  -0.155689103117\n",
      "171  Std  3.27687438389\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "172  Reward  -82.8115180349\n",
      "172  Mean  -0.155777524221\n",
      "172  Std  3.27491455605\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "173  Reward  -83.2331700972\n",
      "173  Mean  -0.155829333163\n",
      "173  Std  3.27293697664\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "174  Reward  -83.4201945625\n",
      "174  Mean  -0.155928020673\n",
      "174  Std  3.27102207175\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "175  Reward  -84.2692286069\n",
      "175  Mean  -0.156027428899\n",
      "175  Std  3.26915305766\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "176  Reward  -84.6372402634\n",
      "176  Mean  -0.156130309005\n",
      "176  Std  3.26737466915\n",
      "-------------------------------\n",
      "Episode finished after 978 timesteps\n",
      "177  Reward  18.1810374016\n",
      "177  Mean  -0.156202148179\n",
      "177  Std  3.26566554856\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "178  Reward  -84.8696317883\n",
      "178  Mean  -0.156138242241\n",
      "178  Std  3.25869065615\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "179  Reward  -82.9865988445\n",
      "179  Mean  -0.156204586377\n",
      "179  Std  3.257058022\n",
      "-------------------------------\n",
      "Episode finished after 775 timesteps\n",
      "180  Reward  35.6187846754\n",
      "180  Mean  -0.156250455315\n",
      "180  Std  3.25538942581\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "181  Reward  -84.3213211662\n",
      "181  Mean  -0.156163692977\n",
      "181  Std  3.24755202585\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "182  Reward  -83.3113894299\n",
      "182  Mean  -0.156260218942\n",
      "182  Std  3.24595582818\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "183  Reward  -81.1074905995\n",
      "183  Mean  -0.156352494361\n",
      "183  Std  3.2443756578\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "184  Reward  -83.4631668358\n",
      "184  Mean  -0.156416656944\n",
      "184  Std  3.24274079349\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "185  Reward  -82.6763204303\n",
      "185  Mean  -0.156530035908\n",
      "185  Std  3.24124896604\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "186  Reward  -85.1478076665\n",
      "186  Mean  -0.15663001957\n",
      "186  Std  3.2397903087\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "187  Reward  -84.3403836964\n",
      "187  Mean  -0.156742214323\n",
      "187  Std  3.23848104439\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "188  Reward  -83.4301237495\n",
      "188  Mean  -0.15677258924\n",
      "188  Std  3.23717615755\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "189  Reward  -85.2464407121\n",
      "189  Mean  -0.156860925725\n",
      "189  Std  3.23587706872\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "190  Reward  -82.2342014568\n",
      "190  Mean  -0.156955316584\n",
      "190  Std  3.23470791343\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "191  Reward  -83.8421420713\n",
      "191  Mean  -0.157044343193\n",
      "191  Std  3.23344481815\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "192  Reward  -84.2417894648\n",
      "192  Mean  -0.15715047554\n",
      "192  Std  3.23230684915\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "193  Reward  -83.1430268852\n",
      "193  Mean  -0.157248221865\n",
      "193  Std  3.23124899117\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "194  Reward  -84.2356502196\n",
      "194  Mean  -0.157361051696\n",
      "194  Std  3.23019019498\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "195  Reward  -84.13529361\n",
      "195  Mean  -0.157454031187\n",
      "195  Std  3.22920734916\n",
      "-------------------------------\n",
      "Episode finished after 879 timesteps\n",
      "196  Reward  26.2376947647\n",
      "196  Mean  -0.157530925239\n",
      "196  Std  3.22828260784\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "197  Reward  -82.7026819711\n",
      "197  Mean  -0.157355887909\n",
      "197  Std  3.22084915327\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "198  Reward  -82.892437032\n",
      "198  Mean  -0.157447498196\n",
      "198  Std  3.21992146035\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "199  Reward  -84.7701009681\n",
      "199  Mean  -0.157538544962\n",
      "199  Std  3.21906748709\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "200  Reward  -84.7306805872\n",
      "200  Mean  -0.157645619279\n",
      "200  Std  3.21832263131\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "201  Reward  -83.2747564194\n",
      "201  Mean  -0.157724080891\n",
      "201  Std  3.21764621954\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "202  Reward  -84.8501627805\n",
      "202  Mean  -0.157781387568\n",
      "202  Std  3.21696877408\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "203  Reward  -84.8199171879\n",
      "203  Mean  -0.15789398654\n",
      "203  Std  3.21639717546\n",
      "-------------------------------\n",
      "Episode finished after 678 timesteps\n",
      "204  Reward  43.5535883457\n",
      "204  Mean  -0.15795588621\n",
      "204  Std  3.21587151168\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "205  Reward  -83.705775456\n",
      "205  Mean  -0.157782632327\n",
      "205  Std  3.20729692467\n",
      "-------------------------------\n",
      "Episode finished after 601 timesteps\n",
      "206  Reward  49.4523124028\n",
      "206  Mean  -0.157870837718\n",
      "206  Std  3.20678901856\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "207  Reward  -84.0871376508\n",
      "207  Mean  -0.157790136255\n",
      "207  Std  3.19785691344\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "208  Reward  -84.3566988197\n",
      "208  Mean  -0.157837837167\n",
      "208  Std  3.19739727917\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "209  Reward  -86.1975420964\n",
      "209  Mean  -0.157945387241\n",
      "209  Std  3.19697946783\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "210  Reward  -82.9498159173\n",
      "210  Mean  -0.158061560881\n",
      "210  Std  3.19673367939\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "211  Reward  -83.6701631992\n",
      "211  Mean  -0.158132721143\n",
      "211  Std  3.19636785363\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "212  Reward  -85.3054112212\n",
      "212  Mean  -0.158226238461\n",
      "212  Std  3.19609679223\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "213  Reward  -83.1367068432\n",
      "213  Mean  -0.158282690482\n",
      "213  Std  3.1959608372\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "214  Reward  -82.6714135498\n",
      "214  Mean  -0.158397660094\n",
      "214  Std  3.19575587509\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "215  Reward  -83.7332523696\n",
      "215  Mean  -0.158474316654\n",
      "215  Std  3.19558370742\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "216  Reward  -84.0721585\n",
      "216  Mean  -0.158553541785\n",
      "216  Std  3.19553950457\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "217  Reward  -81.1895518308\n",
      "217  Mean  -0.158614698182\n",
      "217  Std  3.19556076642\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "218  Reward  -85.1898723323\n",
      "218  Mean  -0.15868356187\n",
      "218  Std  3.1954888816\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "219  Reward  -83.9359854377\n",
      "219  Mean  -0.15880018937\n",
      "219  Std  3.19568356989\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "220  Reward  -83.7545695835\n",
      "220  Mean  -0.158905804025\n",
      "220  Std  3.19588026458\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "221  Reward  -82.4835980315\n",
      "221  Mean  -0.158995391735\n",
      "221  Std  3.19609341692\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "222  Reward  -82.8639481547\n",
      "222  Mean  -0.159095537001\n",
      "222  Std  3.19628666395\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "223  Reward  -84.5419751727\n",
      "223  Mean  -0.159185239794\n",
      "223  Std  3.19660214539\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "224  Reward  -84.9952026773\n",
      "224  Mean  -0.159281506086\n",
      "224  Std  3.19704788716\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "225  Reward  -83.2349402314\n",
      "225  Mean  -0.159352819506\n",
      "225  Std  3.19759513189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "226  Reward  -81.7480377398\n",
      "226  Mean  -0.159420941235\n",
      "226  Std  3.1980835955\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "227  Reward  -82.5540055453\n",
      "227  Mean  -0.159503785211\n",
      "227  Std  3.19852126413\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "228  Reward  -82.1186039505\n",
      "228  Mean  -0.159584510098\n",
      "228  Std  3.19908984062\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "229  Reward  -82.5446948527\n",
      "229  Mean  -0.159661187127\n",
      "229  Std  3.19968882943\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "230  Reward  -83.4790741366\n",
      "230  Mean  -0.159799318723\n",
      "230  Std  3.20037131157\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "231  Reward  -83.5070918\n",
      "231  Mean  -0.159882333252\n",
      "231  Std  3.20115799675\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "232  Reward  -83.960915475\n",
      "232  Mean  -0.15995536224\n",
      "232  Std  3.20202309312\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "233  Reward  -82.6821753633\n",
      "233  Mean  -0.160018502857\n",
      "233  Std  3.20297286453\n",
      "-------------------------------\n",
      "Episode finished after 770 timesteps\n",
      "234  Reward  36.7727160786\n",
      "234  Mean  -0.160084572662\n",
      "234  Std  3.20387716632\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "235  Reward  -83.8312206721\n",
      "235  Mean  -0.159943131563\n",
      "235  Std  3.19526525531\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "236  Reward  -82.7140863325\n",
      "236  Mean  -0.160063335861\n",
      "236  Std  3.1963159157\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "237  Reward  -82.4835861455\n",
      "237  Mean  -0.160160978768\n",
      "237  Std  3.19734178894\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "238  Reward  -84.957533489\n",
      "238  Mean  -0.160235945673\n",
      "238  Std  3.19844287885\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "239  Reward  -84.8976381065\n",
      "239  Mean  -0.160285852804\n",
      "239  Std  3.19976089439\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "240  Reward  -84.6454854341\n",
      "240  Mean  -0.160363157001\n",
      "240  Std  3.20112370571\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "241  Reward  -83.4072231254\n",
      "241  Mean  -0.160428497531\n",
      "241  Std  3.20252803419\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "242  Reward  -81.9981032156\n",
      "242  Mean  -0.160528630851\n",
      "242  Std  3.20389422705\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "243  Reward  -84.3620755868\n",
      "243  Mean  -0.160625413292\n",
      "243  Std  3.205213717\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "244  Reward  -84.8367128404\n",
      "244  Mean  -0.160675814107\n",
      "244  Std  3.20678154965\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "245  Reward  -83.2450199445\n",
      "245  Mean  -0.160817859666\n",
      "245  Std  3.20842694497\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "246  Reward  -84.0702168946\n",
      "246  Mean  -0.160912306085\n",
      "246  Std  3.21001938062\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "247  Reward  -83.8906781595\n",
      "247  Mean  -0.161013789632\n",
      "247  Std  3.21172972743\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "248  Reward  -84.3403861675\n",
      "248  Mean  -0.161087414421\n",
      "248  Std  3.21349967374\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "249  Reward  -83.1907200706\n",
      "249  Mean  -0.161161550811\n",
      "249  Std  3.21539423365\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "250  Reward  -82.2411446408\n",
      "250  Mean  -0.161216111056\n",
      "250  Std  3.2172661458\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "251  Reward  -85.094638366\n",
      "251  Mean  -0.161294580677\n",
      "251  Std  3.2191495161\n",
      "-------------------------------\n",
      "Episode finished after 539 timesteps\n",
      "252  Reward  54.19524173\n",
      "252  Mean  -0.161356901437\n",
      "252  Std  3.22130827749\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "253  Reward  -84.5409726397\n",
      "253  Mean  -0.161169401567\n",
      "253  Std  3.21079737144\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "254  Reward  -83.6197688741\n",
      "254  Mean  -0.161221408231\n",
      "254  Std  3.21295065517\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "255  Reward  -82.474215494\n",
      "255  Mean  -0.161267441051\n",
      "255  Std  3.21510512495\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "256  Reward  -81.1951959238\n",
      "256  Mean  -0.161368840694\n",
      "256  Std  3.21727804941\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "257  Reward  -83.1717333797\n",
      "257  Mean  -0.161462736737\n",
      "257  Std  3.21936529816\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "258  Reward  -83.7011011165\n",
      "258  Mean  -0.161541620826\n",
      "258  Std  3.22162639761\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "259  Reward  -82.8417484216\n",
      "259  Mean  -0.161614997813\n",
      "259  Std  3.22405637812\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "260  Reward  -83.6347383577\n",
      "260  Mean  -0.161701790444\n",
      "260  Std  3.22645235437\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "261  Reward  -84.1750572332\n",
      "261  Mean  -0.161751471938\n",
      "261  Std  3.22901744504\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "262  Reward  -82.6214696665\n",
      "262  Mean  -0.161863552914\n",
      "262  Std  3.23167894248\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "263  Reward  -84.3420982335\n",
      "263  Mean  -0.161904036586\n",
      "263  Std  3.23428845869\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "264  Reward  -82.5290820599\n",
      "264  Mean  -0.161963818641\n",
      "264  Std  3.23710358553\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "265  Reward  -84.3397445847\n",
      "265  Mean  -0.162021783729\n",
      "265  Std  3.23984979707\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "266  Reward  -85.7096132321\n",
      "266  Mean  -0.162109282051\n",
      "266  Std  3.24278108111\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "267  Reward  -84.8575697125\n",
      "267  Mean  -0.162205332661\n",
      "267  Std  3.2459061179\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "268  Reward  -83.5543468989\n",
      "268  Mean  -0.162294274369\n",
      "268  Std  3.24903086784\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "269  Reward  -82.8368147734\n",
      "269  Mean  -0.162400326498\n",
      "269  Std  3.25210221566\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "270  Reward  -82.8575487247\n",
      "270  Mean  -0.162464229142\n",
      "270  Std  3.2551966897\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "271  Reward  -85.028661631\n",
      "271  Mean  -0.162533759807\n",
      "271  Std  3.25836707112\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "272  Reward  -82.900056481\n",
      "272  Mean  -0.162604395097\n",
      "272  Std  3.26180678899\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "273  Reward  -84.3472505614\n",
      "273  Mean  -0.162664704281\n",
      "273  Std  3.26511768996\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "274  Reward  -81.8323980724\n",
      "274  Mean  -0.162797152537\n",
      "274  Std  3.26861224345\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "275  Reward  -83.9691229754\n",
      "275  Mean  -0.162891877338\n",
      "275  Std  3.27196334943\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "276  Reward  -84.2638247698\n",
      "276  Mean  -0.162965688613\n",
      "276  Std  3.27559126477\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "277  Reward  -84.4111808895\n",
      "277  Mean  -0.163015377277\n",
      "277  Std  3.279323696\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "278  Reward  -83.2838279365\n",
      "278  Mean  -0.163115457715\n",
      "278  Std  3.28314063239\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "279  Reward  -83.8469472213\n",
      "279  Mean  -0.163199175458\n",
      "279  Std  3.2868903288\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "280  Reward  -84.3174402048\n",
      "280  Mean  -0.163256708813\n",
      "280  Std  3.29077248274\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "281  Reward  -82.4117359656\n",
      "281  Mean  -0.163354700081\n",
      "281  Std  3.29474079733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "282  Reward  -83.3691493071\n",
      "282  Mean  -0.163454095865\n",
      "282  Std  3.29858714209\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "283  Reward  -84.495191888\n",
      "283  Mean  -0.16359416242\n",
      "283  Std  3.30264652086\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "284  Reward  -85.2013309023\n",
      "284  Mean  -0.163653200175\n",
      "284  Std  3.30691459156\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "285  Reward  -84.0407988523\n",
      "285  Mean  -0.163704228326\n",
      "285  Std  3.31128534099\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "286  Reward  -83.1220672968\n",
      "286  Mean  -0.163814721507\n",
      "286  Std  3.31557330683\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "287  Reward  -83.1553833432\n",
      "287  Mean  -0.163890856768\n",
      "287  Std  3.31990460312\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "288  Reward  -82.7700838241\n",
      "288  Mean  -0.163947639093\n",
      "288  Std  3.32433284921\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "289  Reward  -83.9849737265\n",
      "289  Mean  -0.164037591156\n",
      "289  Std  3.32878757224\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "290  Reward  -85.9636784993\n",
      "290  Mean  -0.164104475832\n",
      "290  Std  3.33339709637\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "291  Reward  -85.7642598688\n",
      "291  Mean  -0.164212102645\n",
      "291  Std  3.33827602737\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "292  Reward  -84.5725966382\n",
      "292  Mean  -0.164282476056\n",
      "292  Std  3.34323178317\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "293  Reward  -84.3526734306\n",
      "293  Mean  -0.164411894578\n",
      "293  Std  3.34810084409\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "294  Reward  -84.4617723428\n",
      "294  Mean  -0.164524215967\n",
      "294  Std  3.35302625082\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "295  Reward  -83.4505603338\n",
      "295  Mean  -0.164616633681\n",
      "295  Std  3.35809203768\n",
      "-------------------------------\n",
      "Episode finished after 511 timesteps\n",
      "296  Reward  57.1930102275\n",
      "296  Mean  -0.164706143437\n",
      "296  Std  3.36312148291\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "297  Reward  -84.2628036649\n",
      "297  Mean  -0.164594971231\n",
      "297  Std  3.35058867363\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "298  Reward  -83.9632380575\n",
      "298  Mean  -0.164661905025\n",
      "298  Std  3.35573728644\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "299  Reward  -84.9891050283\n",
      "299  Mean  -0.164722950922\n",
      "299  Std  3.36093861059\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "300  Reward  -85.6966101734\n",
      "300  Mean  -0.164806731148\n",
      "300  Std  3.36631422974\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "301  Reward  -85.6488319964\n",
      "301  Mean  -0.164897679049\n",
      "301  Std  3.3718468047\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "302  Reward  -83.4742539834\n",
      "302  Mean  -0.164977284579\n",
      "302  Std  3.37754325242\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "303  Reward  -82.9973945849\n",
      "303  Mean  -0.165022398946\n",
      "303  Std  3.38304881233\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "304  Reward  -84.5894945034\n",
      "304  Mean  -0.165068542898\n",
      "304  Std  3.38853863652\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "305  Reward  -86.2481141437\n",
      "305  Mean  -0.165148781946\n",
      "305  Std  3.39434804006\n",
      "-------------------------------\n",
      "Episode finished after 948 timesteps\n",
      "306  Reward  22.1835069273\n",
      "306  Mean  -0.165265290952\n",
      "306  Std  3.40031224789\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "307  Reward  -84.6682488437\n",
      "307  Mean  -0.165139927199\n",
      "307  Std  3.3920295542\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "308  Reward  -85.4921284143\n",
      "308  Mean  -0.165242747433\n",
      "308  Std  3.39790545369\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "309  Reward  -86.2628845169\n",
      "309  Mean  -0.165332328637\n",
      "309  Std  3.40396167412\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "310  Reward  -83.3147746426\n",
      "310  Mean  -0.165406553617\n",
      "310  Std  3.41020826112\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "311  Reward  -84.0362862532\n",
      "311  Mean  -0.165511403375\n",
      "311  Std  3.41618392395\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "312  Reward  -85.4900114363\n",
      "312  Mean  -0.165606364776\n",
      "312  Std  3.42235841746\n",
      "-------------------------------\n",
      "Episode finished after 451 timesteps\n",
      "313  Reward  63.3838178985\n",
      "313  Mean  -0.165704831023\n",
      "313  Std  3.42875137986\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "314  Reward  -84.8799520125\n",
      "314  Mean  -0.165549905328\n",
      "314  Std  3.41477181029\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "315  Reward  -84.5856594605\n",
      "315  Mean  -0.165647706868\n",
      "315  Std  3.4211723011\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "316  Reward  -84.4205570959\n",
      "316  Mean  -0.165715910169\n",
      "316  Std  3.42760192\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "317  Reward  -85.1946156333\n",
      "317  Mean  -0.165797378054\n",
      "317  Std  3.43412170535\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "318  Reward  -82.9023278322\n",
      "318  Mean  -0.165885143382\n",
      "318  Std  3.44078059584\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "319  Reward  -84.7123788574\n",
      "319  Mean  -0.16597893981\n",
      "319  Std  3.44720578759\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "320  Reward  -84.0655363937\n",
      "320  Mean  -0.166030057321\n",
      "320  Std  3.4539774272\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "321  Reward  -85.4529303187\n",
      "321  Mean  -0.166099108929\n",
      "321  Std  3.46074369906\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "322  Reward  -83.9866991911\n",
      "322  Mean  -0.166179376939\n",
      "322  Std  3.46772335555\n",
      "-------------------------------\n",
      "Episode finished after 869 timesteps\n",
      "323  Reward  25.6225324365\n",
      "323  Mean  -0.166260793752\n",
      "323  Std  3.47461566451\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "324  Reward  -84.9041077652\n",
      "324  Mean  -0.166206410531\n",
      "324  Std  3.46545903828\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "325  Reward  -83.7479620007\n",
      "325  Mean  -0.166311873534\n",
      "325  Std  3.47256224786\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "326  Reward  -84.1036212076\n",
      "326  Mean  -0.166365134557\n",
      "326  Std  3.47955784882\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "327  Reward  -84.4574993388\n",
      "327  Mean  -0.166444192307\n",
      "327  Std  3.48672065687\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "328  Reward  -84.395483619\n",
      "328  Mean  -0.166524492538\n",
      "328  Std  3.49398422146\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "329  Reward  -85.6207384405\n",
      "329  Mean  -0.166582796401\n",
      "329  Std  3.50127877745\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "330  Reward  -85.8493787889\n",
      "330  Mean  -0.166652377719\n",
      "330  Std  3.50882688271\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "331  Reward  -86.1551303644\n",
      "331  Mean  -0.166774326815\n",
      "331  Std  3.51646046562\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "332  Reward  -86.3444767984\n",
      "332  Mean  -0.16683159428\n",
      "332  Std  3.52429421319\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "333  Reward  -86.3750728942\n",
      "333  Mean  -0.166915167956\n",
      "333  Std  3.53224714314\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "334  Reward  -85.7892355088\n",
      "334  Mean  -0.166990569055\n",
      "334  Std  3.54029536121\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "335  Reward  -84.0829062688\n",
      "335  Mean  -0.167068805276\n",
      "335  Std  3.54833354096\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "336  Reward  -85.6926987835\n",
      "336  Mean  -0.167145167614\n",
      "336  Std  3.55623142275\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "337  Reward  -85.8858193427\n",
      "337  Mean  -0.167239148817\n",
      "337  Std  3.56436259083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "338  Reward  -84.8076663847\n",
      "338  Mean  -0.167312463891\n",
      "338  Std  3.57264973375\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "339  Reward  -87.0359239737\n",
      "339  Mean  -0.167387103835\n",
      "339  Std  3.58088389697\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "340  Reward  -85.4040306465\n",
      "340  Mean  -0.167468947317\n",
      "340  Std  3.58949973768\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "341  Reward  -84.8485491951\n",
      "341  Mean  -0.167551863811\n",
      "341  Std  3.5979488786\n",
      "-------------------------------\n",
      "Episode finished after 825 timesteps\n",
      "342  Reward  28.8599414479\n",
      "342  Mean  -0.167610001257\n",
      "342  Std  3.60644460659\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "343  Reward  -85.2577750822\n",
      "343  Mean  -0.167523277332\n",
      "343  Std  3.59634527222\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "344  Reward  -85.6977569884\n",
      "344  Mean  -0.167622058531\n",
      "344  Std  3.60494895617\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "345  Reward  -84.6713245934\n",
      "345  Mean  -0.167711664784\n",
      "345  Std  3.61364911596\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "346  Reward  -86.1010646499\n",
      "346  Mean  -0.16778520854\n",
      "346  Std  3.62237630791\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "347  Reward  -86.6603250558\n",
      "347  Mean  -0.167867317059\n",
      "347  Std  3.63132160104\n",
      "-------------------------------\n",
      "Episode finished after 866 timesteps\n",
      "348  Reward  26.9059354864\n",
      "348  Mean  -0.167954775712\n",
      "348  Std  3.64043159877\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "349  Reward  -85.7374184257\n",
      "349  Mean  -0.167867387774\n",
      "349  Std  3.63051431441\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "350  Reward  -86.0099637313\n",
      "350  Mean  -0.16792701232\n",
      "350  Std  3.63961869965\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "351  Reward  -85.440072629\n",
      "351  Mean  -0.168001511897\n",
      "351  Std  3.64884148797\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "352  Reward  -85.7155118585\n",
      "352  Mean  -0.168065971149\n",
      "352  Std  3.65810767842\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "353  Reward  -84.9578280885\n",
      "353  Mean  -0.168145179075\n",
      "353  Std  3.66736219973\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "354  Reward  -86.1984209821\n",
      "354  Mean  -0.168196831217\n",
      "354  Std  3.67675411696\n",
      "-------------------------------\n",
      "Episode finished after 779 timesteps\n",
      "355  Reward  33.2044760225\n",
      "355  Mean  -0.168278295398\n",
      "355  Std  3.68626626873\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "356  Reward  -84.3057024945\n",
      "356  Mean  -0.168141384763\n",
      "356  Std  3.67509427114\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "357  Reward  -83.8895738842\n",
      "357  Mean  -0.168204560364\n",
      "357  Std  3.68436568687\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "358  Reward  -86.0578717487\n",
      "358  Mean  -0.16828075478\n",
      "358  Std  3.69368833565\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "359  Reward  -86.4040079459\n",
      "359  Mean  -0.168341455102\n",
      "359  Std  3.70350246064\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "360  Reward  -83.7771087003\n",
      "360  Mean  -0.168411843744\n",
      "360  Std  3.71337241526\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "361  Reward  -85.8336191803\n",
      "361  Mean  -0.168510447804\n",
      "361  Std  3.72300241111\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "362  Reward  -85.8887584793\n",
      "362  Mean  -0.16858307509\n",
      "362  Std  3.73299690981\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "363  Reward  -86.6216696601\n",
      "363  Mean  -0.168653620398\n",
      "363  Std  3.74312800904\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "364  Reward  -87.6797334764\n",
      "364  Mean  -0.16871318005\n",
      "364  Std  3.75346552703\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "365  Reward  -85.3934416755\n",
      "365  Mean  -0.168811186581\n",
      "365  Std  3.76401433048\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "366  Reward  -85.0190433713\n",
      "366  Mean  -0.168903622921\n",
      "366  Std  3.77423426394\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "367  Reward  -85.7142850336\n",
      "367  Mean  -0.168988165331\n",
      "367  Std  3.784510517\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "368  Reward  -85.1976662502\n",
      "368  Mean  -0.169066901793\n",
      "368  Std  3.794967208\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "369  Reward  -86.1297550025\n",
      "369  Mean  -0.169114813458\n",
      "369  Std  3.80544968166\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "370  Reward  -85.4386711386\n",
      "370  Mean  -0.169166923928\n",
      "370  Std  3.81614866561\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "371  Reward  -86.3342553946\n",
      "371  Mean  -0.169267641549\n",
      "371  Std  3.82681379384\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "372  Reward  -86.646931478\n",
      "372  Mean  -0.169346139262\n",
      "372  Std  3.8376687233\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "373  Reward  -86.1086922414\n",
      "373  Mean  -0.169455572598\n",
      "373  Std  3.84861439124\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "374  Reward  -86.7772532327\n",
      "374  Mean  -0.169522561536\n",
      "374  Std  3.85971609512\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "375  Reward  -86.1597434802\n",
      "375  Mean  -0.169588148766\n",
      "375  Std  3.870901861\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "376  Reward  -86.8697696662\n",
      "376  Mean  -0.169658837336\n",
      "376  Std  3.88203903337\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "377  Reward  -86.9917145431\n",
      "377  Mean  -0.169741949211\n",
      "377  Std  3.89336720686\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "378  Reward  -86.6754695534\n",
      "378  Mean  -0.169817064747\n",
      "378  Std  3.90485228793\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "379  Reward  -86.5053428948\n",
      "379  Mean  -0.169889030492\n",
      "379  Std  3.91637269667\n",
      "-------------------------------\n",
      "Episode finished after 827 timesteps\n",
      "380  Reward  28.674759345\n",
      "380  Mean  -0.169959868558\n",
      "380  Std  3.92799687765\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "381  Reward  -86.9202706524\n",
      "381  Mean  -0.169787136418\n",
      "381  Std  3.91698990434\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "382  Reward  -85.6193406808\n",
      "382  Mean  -0.169852796409\n",
      "382  Std  3.92878799435\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "383  Reward  -86.2366211545\n",
      "383  Mean  -0.169934989829\n",
      "383  Std  3.9404257849\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "384  Reward  -86.6322004999\n",
      "384  Mean  -0.170019517391\n",
      "384  Std  3.9522136888\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "385  Reward  -87.1581699807\n",
      "385  Mean  -0.170105401745\n",
      "385  Std  3.96413292511\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "386  Reward  -83.7443827377\n",
      "386  Mean  -0.170150787863\n",
      "386  Std  3.97628775362\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "387  Reward  -86.4253975048\n",
      "387  Mean  -0.170227999267\n",
      "387  Std  3.98793795249\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "388  Reward  -86.0466213241\n",
      "388  Mean  -0.170291982736\n",
      "388  Std  4.00013533412\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "389  Reward  -87.3962295043\n",
      "389  Mean  -0.170356855574\n",
      "389  Std  4.01222720327\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "390  Reward  -85.3642461349\n",
      "390  Mean  -0.170434899442\n",
      "390  Std  4.02465812597\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "391  Reward  -88.3761363971\n",
      "391  Mean  -0.170516847104\n",
      "391  Std  4.03688974708\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "392  Reward  -87.6959141605\n",
      "392  Mean  -0.170576378953\n",
      "392  Std  4.0497233739\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "393  Reward  -87.0365455855\n",
      "393  Mean  -0.170639340556\n",
      "393  Std  4.06248383422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "394  Reward  -86.9726276571\n",
      "394  Mean  -0.170705957193\n",
      "394  Std  4.07519584687\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "395  Reward  -85.9924956738\n",
      "395  Mean  -0.170765373156\n",
      "395  Std  4.08802458871\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "396  Reward  -87.0246090814\n",
      "396  Mean  -0.170828959784\n",
      "396  Std  4.10069214393\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "397  Reward  -87.159978651\n",
      "397  Mean  -0.170908799806\n",
      "397  Std  4.11362881578\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "398  Reward  -87.4581379449\n",
      "398  Mean  -0.170980344743\n",
      "398  Std  4.12668488313\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "399  Reward  -88.1311486534\n",
      "399  Mean  -0.171052425476\n",
      "399  Std  4.1399202659\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "400  Reward  -87.4344082032\n",
      "400  Mean  -0.171127090047\n",
      "400  Std  4.15333177722\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "401  Reward  -87.4846522389\n",
      "401  Mean  -0.171190432185\n",
      "401  Std  4.16672365771\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "402  Reward  -90.0241682165\n",
      "402  Mean  -0.171276564401\n",
      "402  Std  4.18021682802\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "403  Reward  -88.4567719964\n",
      "403  Mean  -0.171342565118\n",
      "403  Std  4.19425837246\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "404  Reward  -86.6709099519\n",
      "404  Mean  -0.171390525954\n",
      "404  Std  4.20809044354\n",
      "-------------------------------\n",
      "Episode finished after 530 timesteps\n",
      "405  Reward  53.2230991851\n",
      "405  Mean  -0.171440484717\n",
      "405  Std  4.22164245886\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "406  Reward  -88.5200683205\n",
      "406  Mean  -0.17135873078\n",
      "406  Std  4.20465982402\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "407  Reward  -88.2624207665\n",
      "407  Mean  -0.171400262575\n",
      "407  Std  4.21861626115\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "408  Reward  -87.9684510401\n",
      "408  Mean  -0.171464840154\n",
      "408  Std  4.23260836301\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "409  Reward  -88.6206449404\n",
      "409  Mean  -0.17152948085\n",
      "409  Std  4.24662452034\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "410  Reward  -87.4315302733\n",
      "410  Mean  -0.171586234593\n",
      "410  Std  4.2608882688\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "411  Reward  -88.1728271034\n",
      "411  Mean  -0.171641310512\n",
      "411  Std  4.27493669378\n",
      "-------------------------------\n",
      "Episode finished after 824 timesteps\n",
      "412  Reward  29.1362871317\n",
      "412  Mean  -0.171678373389\n",
      "412  Std  4.28934344966\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "413  Reward  -86.4155847003\n",
      "413  Mean  -0.171586966347\n",
      "413  Std  4.2773312374\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "414  Reward  -87.5994860528\n",
      "414  Mean  -0.171666767107\n",
      "414  Std  4.29133752482\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "415  Reward  -87.6721815227\n",
      "415  Mean  -0.171732252297\n",
      "415  Std  4.30562667266\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "416  Reward  -87.6380754276\n",
      "416  Mean  -0.171765423627\n",
      "416  Std  4.32002822389\n",
      "-------------------------------\n",
      "Episode finished after 770 timesteps\n",
      "417  Reward  32.5523965667\n",
      "417  Mean  -0.171831757433\n",
      "417  Std  4.33455676987\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "418  Reward  -87.0398410618\n",
      "418  Mean  -0.171765447266\n",
      "418  Std  4.3216983179\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "419  Reward  -87.5056741157\n",
      "419  Mean  -0.171825653923\n",
      "419  Std  4.3361881436\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "420  Reward  -89.6030332095\n",
      "420  Mean  -0.171880285447\n",
      "420  Std  4.35073973426\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "421  Reward  -87.296663824\n",
      "421  Mean  -0.171934150584\n",
      "421  Std  4.36582784707\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "422  Reward  -88.3684148937\n",
      "422  Mean  -0.171996296479\n",
      "422  Std  4.38051407052\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "423  Reward  -87.3687799763\n",
      "423  Mean  -0.172044521663\n",
      "423  Std  4.39553848378\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "424  Reward  -87.6094235368\n",
      "424  Mean  -0.172098432062\n",
      "424  Std  4.41047722557\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "425  Reward  -88.0185610467\n",
      "425  Mean  -0.172134190056\n",
      "425  Std  4.42553144559\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "426  Reward  -87.5559595507\n",
      "426  Mean  -0.172184377015\n",
      "426  Std  4.44069563577\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "427  Reward  -88.319054458\n",
      "427  Mean  -0.172237667322\n",
      "427  Std  4.45588219955\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "428  Reward  -88.5715182929\n",
      "428  Mean  -0.17230420062\n",
      "428  Std  4.47131789854\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "429  Reward  -88.8086371069\n",
      "429  Mean  -0.172354082199\n",
      "429  Std  4.48691094226\n",
      "-------------------------------\n",
      "Episode finished after 640 timesteps\n",
      "430  Reward  43.5881439475\n",
      "430  Mean  -0.172388973297\n",
      "430  Std  4.50255611074\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "431  Reward  -86.8680319456\n",
      "431  Mean  -0.172336609238\n",
      "431  Std  4.48681105505\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "432  Reward  -86.6320752159\n",
      "432  Mean  -0.172405897922\n",
      "432  Std  4.50212005088\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "433  Reward  -89.0811861757\n",
      "433  Mean  -0.172478338249\n",
      "433  Std  4.5174130677\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "434  Reward  -87.5832180002\n",
      "434  Mean  -0.172526938507\n",
      "434  Std  4.53332266911\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "435  Reward  -88.1868777035\n",
      "435  Mean  -0.172579624314\n",
      "435  Std  4.54897851596\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "436  Reward  -87.2294325684\n",
      "436  Mean  -0.172619241474\n",
      "436  Std  4.56491137725\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "437  Reward  -91.0519938597\n",
      "437  Mean  -0.172659113198\n",
      "437  Std  4.58070734804\n",
      "-------------------------------\n",
      "Episode finished after 876 timesteps\n",
      "438  Reward  22.531528704\n",
      "438  Mean  -0.172748484696\n",
      "438  Std  4.59733059991\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "439  Reward  -88.0100863826\n",
      "439  Mean  -0.172681803434\n",
      "439  Std  4.58648956889\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "440  Reward  -89.6979443195\n",
      "440  Mean  -0.172726110486\n",
      "440  Std  4.60257913495\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "441  Reward  -88.716531536\n",
      "441  Mean  -0.172763447357\n",
      "441  Std  4.61909184121\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "442  Reward  -87.658764716\n",
      "442  Mean  -0.172832811105\n",
      "442  Std  4.63545792976\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "443  Reward  -87.5079837891\n",
      "443  Mean  -0.172891366062\n",
      "443  Std  4.65168633782\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "444  Reward  -88.434135015\n",
      "444  Mean  -0.172948546388\n",
      "444  Std  4.66786675518\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "445  Reward  -89.1794230342\n",
      "445  Mean  -0.173010322893\n",
      "445  Std  4.68440575057\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "446  Reward  -88.8489674241\n",
      "446  Mean  -0.17305561697\n",
      "446  Std  4.70123794216\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "447  Reward  -89.7641392077\n",
      "447  Mean  -0.173095937266\n",
      "447  Std  4.71798794572\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "448  Reward  -89.4610686579\n",
      "448  Mean  -0.173153089121\n",
      "448  Std  4.73504404514\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "449  Reward  -89.1142840505\n",
      "449  Mean  -0.173203381442\n",
      "449  Std  4.75205708597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "450  Reward  -88.4897175275\n",
      "450  Mean  -0.173257442774\n",
      "450  Std  4.76908257289\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "451  Reward  -89.5826251114\n",
      "451  Mean  -0.173323470296\n",
      "451  Std  4.78595593208\n",
      "-------------------------------\n",
      "Episode finished after 1000 timesteps\n",
      "452  Reward  -89.9517997251\n",
      "452  Mean  -0.173391781714\n",
      "452  Std  4.80316123043\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 100000\n",
    "horizon = 1000\n",
    "for i_episode in range(n_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    global time\n",
    "    timee = i_episode\n",
    "    actions = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    rewards = []\n",
    "    time = 0\n",
    "    for t in range(horizon):\n",
    "        time+=1\n",
    "        env.render()\n",
    "        ########################################\n",
    "        action, mean, std = choose_action(state)\n",
    "        actions.append(action)\n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "        ########################################\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    reinforce(actions, means, stds, rewards)\n",
    "    \n",
    "    print(\"-------------------------------\")\n",
    "    print(\"Episode finished after {} timesteps\".format(time+1))\n",
    "    print(i_episode, \" Reward \", np.array(rewards).sum())\n",
    "    print(i_episode, \" Mean \", np.array(means).mean())\n",
    "    print(i_episode, \" Std \", std_transformation(np.array(stds)).mean())\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "value_nn = FerchNN.NN_Network(input_size = env.observation_space.shape[0], lr =0.000001, output_size = 1, activation = \"linear\", loss = \"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_episodes = 100000\n",
    "horizon = 1000\n",
    "gamma = 0.99\n",
    "for i_episode in range(n_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    \n",
    "    actions = []\n",
    "    means = []\n",
    "    stds = []\n",
    "    rewards = []\n",
    "    time = 0\n",
    "    for t in range(horizon):\n",
    "        time+=1\n",
    "        env.render()\n",
    "        ########################################\n",
    "        action, mean, std = choose_action(state)\n",
    "        actions.append(action)\n",
    "        means.append(mean)\n",
    "        stds.append(std)\n",
    "        ########################################\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        \n",
    "        V_next = value_nn.predict(next_state)\n",
    "        \n",
    "        if done:\n",
    "            V_next = np.zeros(1)\n",
    "        \n",
    "        TD = reward+ gamma*V_next - value_nn.predict(state)\n",
    "        \n",
    "        value_nn.fit(state, reward+ gamma*V_next)\n",
    "        \n",
    "        mean_gradient, std_gradient  = compute_gradient(action,TD, mean, std)\n",
    "        mean_nn.fit(None,None, dloss = mean_gradient)\n",
    "        std_nn.fit(None, None, dloss = std_gradient)\n",
    "        \n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    print(\"-------------------------------\")\n",
    "    print(\"Episode finished after {} timesteps\".format(time+1))\n",
    "    print(i_episode, \" Reward \", np.array(rewards).sum())\n",
    "    print(i_episode, \" Mean \", np.array(means).mean())\n",
    "    print(i_episode, \" Std \", std_transformation(np.array(stds)).mean())\n",
    "    \n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
